{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Validation \n",
    "\n",
    "- Takes train and val scan IDs (i.e. individuals) from cross-val generator\n",
    "- Uses train generator to sample slices from case and control train samples. Performs data augmentations (optional)\n",
    "- Trains the model for a single hyper-param config. Reports performance on validation slices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "sys.path.append('../')\n",
    "from lib.training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = '' # local or Dropbox location for dicoms\n",
    "\n",
    "metadata_file = '' # scan IDs, slice paths, lung partition, and demographic info\n",
    "CV_dict_file = '' # k-fold generator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_CV_folds = True\n",
    "RANDOM_SEED = 153\n",
    "n_folds = 2\n",
    "\n",
    "group_col = 'DX' #'partition_id' or 'DX'\n",
    "case_label = 1\n",
    "control_label = 0\n",
    "\n",
    "slice_shape = (10,10)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CV folds to divide scans IDs balanced by groups...\n",
      "TRAIN: [ 0  2  5  7  8  9 11 12 13 18 22 24 25 31 32 33 34 37 38 39 41 43 44 48\n",
      " 49 50 52 53 55 56 57 61 62 63 64 65 67 68 69 70 72 74 80 82 83 85 89 93\n",
      " 97 99] TEST: [ 1  3  4  6 10 14 15 16 17 19 20 21 23 26 27 28 29 30 35 36 40 42 45 46\n",
      " 47 51 54 58 59 60 66 71 73 75 76 77 78 79 81 84 86 87 88 90 91 92 94 95\n",
      " 96 98]\n",
      "1.0    75\n",
      "0.0    75\n",
      "Name: slice_label, dtype: int64\n",
      "TRAIN: [ 1  3  4  6 10 14 15 16 17 19 20 21 23 26 27 28 29 30 35 36 40 42 45 46\n",
      " 47 51 54 58 59 60 66 71 73 75 76 77 78 79 81 84 86 87 88 90 91 92 94 95\n",
      " 96 98] TEST: [ 0  2  5  7  8  9 11 12 13 18 22 24 25 31 32 33 34 37 38 39 41 43 44 48\n",
      " 49 50 52 53 55 56 57 61 62 63 64 65 67 68 69 70 72 74 80 82 83 85 89 93\n",
      " 97 99]\n",
      "1.0    75\n",
      "0.0    75\n",
      "Name: slice_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dummy data\n",
    "metadata_df=pd.DataFrame()\n",
    "n_scans = 100\n",
    "n_parts = 3\n",
    "metadata_df['CT_id'] = np.repeat(range(n_scans),n_parts)\n",
    "metadata_df['DX'] = list(np.repeat(np.zeros(n_scans//2),n_parts)) + list(np.repeat(np.ones(n_scans//2),n_parts))\n",
    "metadata_df['slice_label'] = metadata_df['DX'] #ideal case\n",
    "metadata_df['slice_id']= range(n_scans*n_parts)\n",
    "\n",
    "# Real data\n",
    "# metadata_df  = pd.read_pickle(metadata_file)\n",
    "# n_scans = len(metadata_df['CT_id'].unique())\n",
    "# n_slices = len(np.hstack(metadata_df['slice_ids'].values))\n",
    "# n_partitions = len(metadata_df['partition_id'].unique())\n",
    "# n_cases = len(metadata_df[metadata_df[group_col]==case_label])\n",
    "# n_controls = len(metadata_df[metadata_df[group_col]==control_label])\n",
    "\n",
    "# print('Number of total scans: {}, slices: {}, partitions: {}\\ncases: {}, controls: {}'.format())\n",
    "if not generate_CV_folds:\n",
    "    CV_index_dict = pickle.load(open(CV_dict_file, \"rb\"))\n",
    "    \n",
    "else:\n",
    "    print('Generating CV folds to divide scans IDs balanced by groups...')\n",
    "    CV_df = metadata_df[['CT_id',group_col]].drop_duplicates()\n",
    "\n",
    "    X = CV_df['CT_id'].values\n",
    "    y = CV_df[group_col].values\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, random_state=RANDOM_SEED, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    CV_index_dict = {}\n",
    "    cv = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        slice_labels = metadata_df[metadata_df['CT_id'].isin(train_index)]['slice_label'].value_counts()\n",
    "        print(slice_labels)\n",
    "        CV_index_dict[cv] = {'train':train_index, 'test':test_index}\n",
    "        cv = cv + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train and test slice samples for a CV fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scans\n",
      "train: 40, val: 10, test: 50\n",
      "number of slice samples in a train batch: 120\n",
      "number of slice samples in a val batch: 30\n",
      "number of slice samples in a test batch: 150\n"
     ]
    }
   ],
   "source": [
    "cv_idx = 0 \n",
    "val_subset_frac = 0.2 #this is a subset within the train samples that is used to validate and select hyper-params\n",
    "\n",
    "train_plus_val_index = CV_index_dict[cv_idx]['train']\n",
    "train_sampx = int((1-val_subset_frac)*len(train_plus_val_index))\n",
    "train_index = train_plus_val_index[:train_sampx]\n",
    "val_index = train_plus_val_index[train_sampx:]\n",
    "\n",
    "test_index = CV_index_dict[cv_idx]['test']\n",
    "\n",
    "print('Number of scans\\ntrain: {}, val: {}, test: {}'.format(len(train_index),len(val_index),len(test_index)))\n",
    "\n",
    "# CT IDs\n",
    "X_train, X_val, X_test = X[train_index], X[val_index], X[test_index]\n",
    "y_train, y_val, y_test = y[train_index], y[val_index], y[test_index]\n",
    "\n",
    "## Train generator\n",
    "data_subset = 'train'\n",
    "train_gen = batch_generator(X_train, metadata_df, data_subset, slice_shape, num_classes)\n",
    "\n",
    "batch = next(train_gen)\n",
    "print('number of slice samples in a {} batch: {}'.format(data_subset, len(batch[1])))\n",
    "\n",
    "## Val generator\n",
    "data_subset = 'val'\n",
    "val_gen = batch_generator(X_val, metadata_df, data_subset, slice_shape, num_classes)\n",
    "\n",
    "batch = next(val_gen)\n",
    "print('number of slice samples in a {} batch: {}'.format(data_subset, len(batch[1])))\n",
    "\n",
    "## Test generator\n",
    "data_subset = 'test'\n",
    "test_gen = batch_generator(X_test, metadata_df, data_subset, slice_shape, num_classes)\n",
    "\n",
    "batch = next(test_gen)\n",
    "print('number of slice samples in a {} batch: {}'.format(data_subset, len(batch[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nikhil/anaconda3/envs/napari/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 8, 8, 8)           80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 16)          1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "input_shape = (slice_shape[0], slice_shape[1], 1)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6947 - acc: 0.5442 - val_loss: 0.8973 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.6672 - acc: 0.6192 - val_loss: 1.0201 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.6690 - acc: 0.6225 - val_loss: 0.9601 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.6652 - acc: 0.6250 - val_loss: 0.9230 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.6608 - acc: 0.6250 - val_loss: 0.9390 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.6629 - acc: 0.6250 - val_loss: 0.9659 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.6698 - acc: 0.6250 - val_loss: 0.9371 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.6644 - acc: 0.6250 - val_loss: 0.9240 - val_acc: 0.0000e+00\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.6675 - acc: 0.6250 - val_loss: 0.9352 - val_acc: 0.0000e+00\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.6671 - acc: 0.6250 - val_loss: 0.9560 - val_acc: 0.0000e+00\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.6696 - acc: 0.6250 - val_loss: 0.9206 - val_acc: 0.0000e+00\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.6649 - acc: 0.6250 - val_loss: 0.9271 - val_acc: 0.0000e+00\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.6639 - acc: 0.6250 - val_loss: 0.9565 - val_acc: 0.0000e+00\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.6613 - acc: 0.6250 - val_loss: 0.9701 - val_acc: 0.0000e+00\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.6663 - acc: 0.6250 - val_loss: 0.9553 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5b9c399b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs, steps_per_epoch=10, validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model (at the moment on dummy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7207905530929566\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_gen, verbose=0, steps=10)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
